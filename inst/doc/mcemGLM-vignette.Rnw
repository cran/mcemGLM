% \VignetteIndexEntry{An R Package to Fit Generalized Linear Mixed Models}
% \VignetteDepends{mcemGLM}

\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{setspace}

\usepackage{geometry} 
\geometry{hmargin=3.0cm,vmargin={3.5cm,3.5cm},nohead,footskip=0.5in}
\doublespacing

\newcommand{\ex}{\textrm{E}}
\newcommand{\var}{\textrm{Var}}
\newcommand{\cov}{\textrm{Cov}}
\newcommand{\N}{\textrm{N}}
\newcommand {\real} {\mathbb{R}}

\author{Felipe Acosta Archila}
\title{The mcemGLM package}

\begin{document}
\maketitle

\SweaveOpts{concordance=TRUE}
\section{A Generalized Linear Mixed Model}
We start by assuming that we observe a vector of data $Y = (Y_1,\dots,Y_n)$ corresponding to a probability model that depends on a $(p + l)$-dimensional parameter vector $\theta$, a known $n \times p$ fixed effects design matrix $X$, a known $n \times k$ known random effects design matrix $Z$ , and a $k$-dimensional vector of unobservable random effects $U$.

Let $\theta$ consist of $p$ fixed effects coefficients $\beta = (\beta_1, \dots, \beta_p)^T$ and $l$ variance parameters, $\sigma^2 = (\sigma_1^2,\dots,\sigma_l^2)^T$, associated to the random effects $U$. Our goal is to find maximum likelihood estimates (MLEs) for the $(p + l)$-dimensional parameter $\theta$ in a space $\Theta$ for a generalized linear mixed model.

We assume that the expected variable of $Y_i$, can be written as a linear combination of the observable and unobservable variables through a bijective ``link'' function $g$. Let $X_i$ and $Z_i$ be the $i$th rows of the matrices $X$ and $Z$, and let $\ex(Y_i|U=u) = \mu_i$. Then
\[
  g(\mu_i) = X_i\, \beta + Z_i\, u, \, \textrm{ for } i = 1, \dots, n.
\]

Let $U = (U_1^T, \dots, U_l^T)^T$, and $Z = (Z_1 \cdots Z_l)$ a decomposition for the vector $U$ and the matrix $Z$. We assume that $U_i$ is a $k_i$-dimensional vector with $\sum_i^l k_i = k$. Furthermore we assume that $U_i$ has a known distribution with variance that depends on the parameter $\sigma_i^2$. In general let $\mu = (\mu_1,\dots,\mu_n)$ and let $g(\mu)$ denote the element-wise evaluation of $g$ on the vector $\mu$, then we can write mean our model as
\begin{align}
\label{link} g(\mu) = X\, \beta + \sum_{i = 1}^l Z_i\, u_i.
\end{align}

Let $h_U(u)$ be the probability density function of $U$. We assume that conditional on $U$, the data is generated from a probability model with probability mass function $f(Y|\theta, X, Z, U)$ and that we can write its likelihood function in terms of $\mu = g^{-1}(X\, \beta + \sum_{i = 1}^l Z_i\, u_i)$, and $\sigma^2$. Defining the model this way yields to the following likelihood functions:
\begin{enumerate}
\item A complete likelihood function:
  \begin{align}
    \label{complete}  L(\theta | X, Z, U) = f(y, u|\theta, X, Z) = f_Y(y|\theta, X, Z, u)\, h_U(u|\theta).
  \end{align}
\item And a marginal likelihood function:
  \begin{align}
    \label{marginal} L(\theta | X, Z) = \int_{\real^l} f(Y|\theta, X, Z, U)\, h_U(u|\theta) du.
  \end{align}
\end{enumerate}

Since the vector $U$ is not observable we need to obtain the MLEs from \ref{marginal}. This means that before maximizing the likelihood function we need to integrate out the vector of random effects.

The \texttt{mcemGLM} package focuses on three types of models for the marginal data:
\begin{enumerate}
  \item Bernoulli data. We say that $Y_i \overset{iid}{\sim} $ Bernoulli$(p_i)$, for $i = 1,\dots,n$, with $0 < p_i < 1$, if $Y_i$ has probability mass function
  \[
    f(y_i) = p_i^{y_i}(1-p_i)^{1-y_i},\, \textrm{ for } y_i=0,1.
  \]
  With $\ex(Y_i) = p_i$, $\var(Y_i) = p_i(1-p_i)$, and $g(p_i)=\log(p_i/(1-p_i))$.
  
  \item Poisson data. We say that $Y_i \overset{iid}{\sim} $ Poisson$(\mu_i)$ for $i = 1,\dots,n$, if  $Y_i$ has probability mass function
  \[
    f(y_i) = e^{-\mu_i}\,\frac{\mu_i^{y_i}}{y_i!},\, \textrm{ for } y_i=0,1,2,\dots
  \]
  With $\ex(Y_i) = \mu_i$, $\var(Y_i) = \mu_i$, and $g(\mu_i)=\log(\mu_i)$.
  
  \item Negative binomial data. We say that $Y_i \overset{iid}{\sim} $ neg-binom$(\mu_i, \alpha)$, for $i = 1,\dots,n$, with $\mu_i > 0$, and $\alpha > 0$, if $Y_i$ has probability mass function
  \[
    f(y_i) = \dfrac{\Gamma(y_i + \alpha)}{\Gamma(\alpha)\,y_i!}\left(\dfrac{\alpha}{\mu_i + \alpha}\right)^\alpha\left(\dfrac{\mu_i}{\mu_i + \alpha}\right)^{y_i},\, \textrm{ for } y_i=0,1,2,\dots
  \]
  With $\ex(Y_i) = \mu_i$, $\var(Y_i) = \mu_i + \mu_i^2/\alpha$, and $g(\mu_i)=\log(\mu_i)$. 
  
  The expectation and variance of $Y_i$ can be found easily by using iterated expectation with respect to a random variable $M$ distributed gamma with shape parameter $\alpha$, and rate parameter $\alpha/\mu$ and setting $Y_i|M=m \sim $ Poisson$(m)$. 
  
  By using this definition of the distribution of $Y_i$ we can treat the parameter $\alpha$ as the amount of over-dispersion with respect to the Poisson distribution. The value $\alpha = \infty$ corresponds to no over-dispersion.
  
  By introducing $\alpha$ to the model notice that we need to estimate this extra parameter in addition to $\beta$ and $\sigma^2$.
\end{enumerate}

In addition to the model selection the \texttt{mcemGLM} package allows to specify two types of random effects. Let $I_{k}$ be an $n \times n$ identity matrix, $\N_n(\mu, \Sigma)$ an $n$-dimensional multivariate normal distribution with mean vector $\mu$ and covariance matrix $\Sigma$, and $t_{n}(\nu, \mu, \Sigma)$ an $n$-dimensional multivariate $t$ distribution with $\nu$ degrees of freedom, location vector $\mu$ and scale matrix $\Sigma$.
\begin{enumerate}
\item Normal distribution. We set $U_i \sim \N_{k_i}(0,\, \sigma_i^2 I_{k_i})$ for $i = 1,\dots,l$, and set the $U_i$s to be jointly independent.
\item t distribution with known degrees of freedom $\nu$. We set $U_i \sim t_{k_i}(\nu,\, 0,\, \sigma_i^2 I_{k_i})$ for $i = 1,\dots,l$, and set the $U_i$s to be jointly independent.
\end{enumerate}

\section{The MCEM algorithm}
The MCEM algorithm is a modification of the EM algorithm. The later assumes two sets of data an observed data set $Y$ and a set of missing data $U$.

The EM algorithm estimates the MLEs by an iterative algorithm. Let $\theta^{(t)}$ denote the current estimate at the $i$th iteration. Let 
\begin{align}
\label{q-fun} Q(\theta, \theta^{(t)}) = \ex\left[\log f(y, u| \theta, X, U) | y, \theta^{(t)}\right].
\end{align}

The next value, $\theta^{(t + 1)}$, is found by maximizing \ref{q-fun} with respect to $\theta$. The expectation in \ref{q-fun} is taken with respect to $f(u|y, \theta, X, Z)$ hence if we want to obtain its closed form we need $f(y, u|\theta, X, Z)$ and $f_Y(y|\theta, X, Z)$. The later function is not available for the models we are considering, therefore we need to resort to a numerical method to calculate this expectation.
% \begin{align*}
%   \theta^{(t+1)} = \arg\underset{\theta}{\max}\, Q(\theta, \theta^{(t)}).
% \end{align*}

The solution implemented in the \texttt{mcemGLM} package is to estimate \ref{q-fun} via a Markov chain Monte Carlo (MCMC) step. This works by obtaining a sample $u_{t, 1}, \dots, u_{t, m}$ from a Markov chain with stationary distribution $f(u|y,\theta,X,Z)$ and then maximizing
\begin{align}
  \label{q-fun-mc} \widehat Q(\theta) = \sum_{j = 1}^m \log f(y, u_{t, j}| \theta, X, Z)
\end{align}
with respect to $\theta$ to obtain $\theta^{(t+1)}$.

The algorithm is run until a termination condition has been reached or the maximum number of iterations has been done.

\section{The mcemGLM package}

The package runs through the following steps:
\begin{enumerate}
\item Choose a starting value. The default method is to fit a model without random effects and using the MLEs of the fixed coefficients as starting values for $\beta$. For $\sigma$ we set a predefined value of 5.
\item Obtain the sample $u_{t, 1}, \dots, u_{t, m}$. This is done by using a Metropolis--Hastings algorithm that uses a multivariate normal random variable as its proposal. The standard deviation vector of the proposal distribution is chosen by performing an auto--tuning step before the first iteration. After each iteration the rejection rate of the chain is checked and if it is either too large (> 0.4)  or to small (< 0.1) the package performs an auto-tuning step before the next iteration.
\item After obtaining the sample \ref{q-fun-mc} is maximized with respect all the parameters using the \texttt{trust} function from the \texttt{trust} package. The maximizers are set as the current value of the estimator of the MLEs.
\item Steps 2 and 3 are repeated until the condition
  \[
    \underset{i}{\max}\left\{\dfrac{|\theta_i^{(t)} - \theta_i^{(t - 1)}|}{|\theta_i^{(t)}| + \delta}\right\} < \epsilon
  \]
  for specified values of $\delta$ and $\epsilon$ is met three consecutive times or a maximum number of iterations have been performed. 
  
  The default values in the package are $\delta = 0.05$ and $\epsilon = 0.01$ but these can be easily changed by the user. The default number of iterations is 80 and this value can also be changed by the user.
\item After terminating the iterative process another sample from the random effects is obtained to estimate the information matrix of the model.
\end{enumerate}

\section{Using the mcemGLM package}
<<echo=TRUE>>=
require(mcemGLM)
data("simData.rdata")
@
<<echo=FALSE>>=
set.seed(23786)
simData$count <- simData$count+rpois(200, 3)
simData$count2 <- simData$count*(1+rpois(200, 2))
@
<<>>=
head(simData)
summary(simData)
@

The data consist of three fixed effects, $x1$, $x2$, and $x3$. The first two fixed effects are continuous and $x3$ is a factor with three levels. There are three variables that we can use as variance components $z1$ (5 levels), $z2$ (4 levels), and $z3$ (3 levels.) The component $z2$ can be nested within $z1$ and $z3$ is crossed with these.

First we will consider a simple model based on this data using \verb obs  as the binary response.

\subsection{A simple model}
We will fit a model with one variance component, $z3$ and we will consider $z1$ as a fixed effect along with $x1$.

The main model arguments for the \verb mcemGLMM  function are \verb fixed  and \verb random.  These specify the fixed and random effects of the model. The response must be included in the \verb fixed  argument. In this first example we are considering $x1$ and $z1$ as fixed and $z3$ as random. We can fit this model with the following command:
<<echo=TRUE>>=
fit0 <- mcemGLMM(fixed = obs ~ x1 + z1, 
                random = ~0+z2, 
                  data = simData, 
                family = "bernoulli", 
                vcDist = "normal")
@
The rest of the used arguments are:
\begin{itemize}
\item \verb data:  argument contains the name of the data frame with the data.
\item \verb family:  argument specifies the type of model to be fitted. The options are ``bernoulli'' for logistic regression, ``poisson'' for Poisson count regression, and ``negbinom'' for negative binomial count  regression.
\item \verb vcDist:  argument specifies the distribution of the random effects. The option are ``normal'', and ``t''. In case of $t$ random effects an extra argument with the degrees of freedom must be supplied.
\end{itemize}

We can start by taking a look at the coefficient and variance estimates with the \verb summary  command:
<<echo=TRUE>>=
summary(fit0)
@
We first get a print of the original call used to fit the model. The summary print out has two tables. The first table shows the estimates, standard errors and $z$ tests for the fixed effect coefficients. While the second table contains the same information but for the variance estimates.

Now we can look at an ANOVA table based on Wald tests.
<<echo=TRUE>>=
anova(fit0)
@
Each line corresponds to a test on the coefficients that are related to each variable. In the case of a continuous variable or a binary this is equivalent to the $z$ test performed with \verb summary.  When a categorical variable has more than two categories \verb anova  will test run a chi--squared test on all the coefficients that are related to that variable. In this case the chi--squared test for $z1$ tests if the corresponding coefficients for \verb D2 , \verb  D3 , \verb D4 , and \verb D5  are both equal to zero.

We can run multiple comparison tests for the levels of $z1$. First we need to create a contrast matrix with each row representing a contrast that we want to test. In this case
<<echo=TRUE>>=
ctr0 <- rbind("D1 - D2" = c(0, 0,-1, 0, 0, 0),
              "D1 - D3" = c(0, 0, 0,-1, 0, 0),
              "D1 - D4" = c(0, 0, 0, 0,-1, 0),
              "D1 - D5" = c(0, 0, 0, 0, 0,-1),
              "D2 - D3" = c(0, 0, 1,-1, 0, 0),
              "D2 - D4" = c(0, 0, 1, 0,-1, 0),
              "D2 - D5" = c(0, 0, 1, 0, 0,-1),
              "D3 - D4" = c(0, 0, 0, 1,-1, 0),
              "D3 - D5" = c(0, 0, 0, 1, 0,-1),
              "D4 - D5" = c(0, 0, 0, 0, 1,-1))
@
Notice that rows one and two are the contrasts that that compare the baseline, \verb D1 , to the other levels, hence these will have equivalent test statistics as those obtained in \verb summary.  However \verb contrasts.mcemGLMM  accounts for multiple comparisons by adjusting the $p$--values via Bonferroni correction therefore it is possible to obtain significance in \verb summary  and not in \verb contrasts.mcemGLMM  since this $p$--value will likely be larger.
<<echo=TRUE>>=
contrasts.mcemGLMM(object = fit0, ctr.mat = ctr0)
@

For this simple model it is possible to plot the predicted probabilities for each level of \verb z1 . These estimates correspond to the population means, i.e., the random effects have been set to zero. Figure \ref{fit0-fitted} shows the plots of the fitted probabilities as a function of $x1$ for the different levels of $z1$.
\begin{figure}[ht]
\centering
\label{fit0-fitted}
<<echo=TRUE, fig=TRUE>>=
plot(simData$x1, predict(fit0, type = "response"), col = simData$z1, xlab = "x1")
@
\caption{Fitted probabilities. Each color represent a level of $z1$}
\end{figure}

We can calculate the Pearson and deviance residuals of the model with the \texttt{residuals} command. Figures \ref{fit0-deviance} and \ref{fit0-pearson} shows these plots.
\begin{figure}[ht]
\label{fit0-deviance}
\centering
<<echo=TRUE, fig=TRUE>>=
plot(simData$x1, residuals(fit0, type = "deviance"))
@
\caption{Deviance residuals}
\end{figure}

\begin{figure}[ht]
\label{fit0-pearson}
\centering
<<echo=TRUE, fig=TRUE>>=
plot(simData$x1, residuals(fit0, type = "pearson"))
@
\caption{Pearson residuals}
\end{figure}

To assess convergence we can look at trace plots of the MLE estimates and the value of the loglikelihood function across the EM iterations. These are stored in the \verb mcemGLMM  object returned by the function the \verb mcemGLMM  function on the fields \verb+mcemEST+ and \verb loglikeVal . Figure \ref{fig-fit0-mle} shows trace plots at each EM iteration of these quantities.

\begin{figure}[ht]
\label{fig-fit0-mle}
\centering
<<echo=TRUE, fig=TRUE, width=8>>=
par(mfrow = c(1, 2))
ts.plot(fit0$mcemEST, main = "MLEs estimates", 
        xlab = "EM Iteration", ylab = "MLE value")
ts.plot(fit0$loglikeVal, main = "Loglikelihood values", 
        xlab = "EM iteration", ylab = "Likelihood")
@
\caption{MLEs estimates (left) and Loglikelihood value (right) after each EM iteration.}
\end{figure}

We can also take a look at the trace plots of the Markov chain used to estimate the $Q$ function. Since this approximates an integral of dimension equal to the number of random effects it might not be practical to look at all the chains. The last MCMC step is saved on the field \verb randeff  as a matrix. Each column of this matrix corresponds to one random effect.

\begin{figure}
\centering
\label{fit0-rand}
<<echo=TRUE, fig=TRUE>>=
ts.plot(fit0$randeff[, 1], xlab = "MCMC iteration")
@
\caption{Trace plot for MCMC output for the first random effect.}
\end{figure}

This matrix can be used to get predictions of the observed random effects.
<<echo=TRUE>>=
colMeans(fit0$randeff)
@

To see the sampling on the loglikelihood function we can plot the values of the complete loglikelihood at each MCMC step of the last EM iteration. These values are stored in the \verb+loglikeMCMC+ field of the \verb+mcemGLMM+ object.
\begin{figure}
\centering
\label{fit0-rand}
<<echo=TRUE, fig=TRUE>>=
ts.plot(fit0$loglikeMCMC, xlab = "MCMC iteration")
@
\caption{Trace plot for the complete loglikelihood function.}
\end{figure}

\clearpage
\subsection{Fitting a more complex model}
To specify more than one random effect we need to put them into a list and state that there is no intercept for that effect. In case of nested random effects if labels are repeated across it is necessary to fit the lower level by using the interaction with the upper level.

In this specific example, the labels for $z2$, "1", "2", "3", and "4", are used for each level of $z1$. If the labels of $z2$ are unique within $z1$ it is not necessary to use the interaction term. However it is recommended to use the interaction form for the sake of clarity in the model statement.
<<echo=TRUE>>=
fit1 <- mcemGLMM(fixed = obs ~ x1 + x2 + x3, 
                random = list(~0+z1, ~0+z1:z2), 
                  data = simData, 
                family = "bernoulli", 
                vcDist = "t", 
                df = c(5, 5))
@
The \verb df  argument specifies the degrees of freedom for each variance component in \verb random.  If \verb vcDist  is ``normal'' this argument is not needed.

We can look at the summary and ANOVA of the model as before
<<echo=TRUE>>=
summary(fit1)
anova(fit1)
@

We can run multiple comparison tests for the levels of $x3$ as before
<<echo=TRUE>>=
ctr1 <- rbind(   "blue - red" = c(0, 0, 0,-1, 0),
              "blue - yellow" = c(0, 0, 0, 0,-1),
               "red - yellow" = c(0, 0, 0, 1,-1))
contrasts.mcemGLMM(object = fit1, ctr.mat = ctr1)
@

Instead of performing a Wald test to test a fixed effect it is possible to run a likelihood ratio test between two nested models. First we will fit a model without \verb x3 :
<<echo=TRUE>>=
fit2 <- mcemGLMM(fixed = obs ~ x1 + x2, 
                random = list(~0+z1, ~0+z1:z2), 
                  data = simData, 
                family = "bernoulli", 
                vcDist = "t", 
                    df = c(5, 5),
                controlEM = list(EMit = 3))
@
Now we can use the \verb anova  command to run the likelihood ratio test
<<echo=TRUE>>=
anova(fit1, fit2)
@

\subsection{A Poisson model}
To fit a Poisson model we only need to change the \verb family  argument in the \verb mcemGLMM  command. As an example we will use the \verb count  variable in \verb simData .
<<echo=TRUE>>=
fit3 <- mcemGLMM(fixed = count ~ x1 + x2 + x3, 
                random = list(~0+z2), 
                  data = simData, 
                family = "poisson", 
                vcDist = "normal")
@
All the previous methods are available for this type of model.
<<echo=TRUE>>=
summary(fit3)
anova(fit3)
contrasts.mcemGLMM(object = fit3, ctr.mat = ctr1)
@

\begin{figure}[ht]
\label{fit3-fitted}
\centering
<<echo=TRUE, fig=TRUE>>=
plot(simData$x1, predict(fit3), 
     main = "Predicted response values", xlab = "x1")
@
\caption{Predicted values as a function of $x1$.}
\end{figure}

\begin{figure}[ht]
\label{fig-fit3-mle}
\centering
<<echo=TRUE, fig=TRUE, width=8>>=
par(mfrow = c(1, 2))
ts.plot(fit3$mcemEST, main = "MLEs estimates", 
        xlab = "EM Iteration", ylab = "MLE value")
ts.plot(fit3$loglikeVal, main = "Loglikelihood values", 
        xlab = "EM iteration", ylab = "Likelihood")
@
\caption{MLEs estimates (left) and Loglikelihood value (right) after each EM iteration.}
\end{figure}


\subsection{A negative binomial model}
To fit a negative binomial model we need to specify the \verb family  argument to \verb "negbinom" . All the previous methods ara available for this model. When we look at the summary of this model we get an estimate of the overdispersion parameter and its standard error.
<<echo=TRUE>>=
fit4 <- mcemGLMM(fixed = count2 ~ x1 + x2 + x3, 
                random = list(~0+z1, ~0+z1:z2), 
                  data = simData, 
                family = "negbinom", 
                vcDist = "normal")
summary(fit4)
anova(fit4)
contrasts.mcemGLMM(object = fit4, ctr.mat = ctr1)
@

\begin{figure}[ht]
\centering
<<echo=TRUE, fig=TRUE>>=
plot(simData$x1, predict(fit4))
@
\end{figure}

\begin{figure}
\label{fit4-mle}
\centering
<<echo=TRUE, fig=TRUE, width=7>>=
par(mfrow = c(1, 2))
ts.plot(fit4$mcemEST, main = "MLEs estimates", 
        xlab = "EM Iteration", ylab = "MLE value")
ts.plot(fit4$loglikeVal, main = "Likelihood value", 
        xlab = "EM Iteration", ylab = "Likelihood")
@
\caption{MLEs estimates and loglikelihood value after each EM iteration.}
\end{figure}
% 
% <<echo=TRUE>>=
% # colMeans(fit4$randeff)
% @
% 
\end{document}
